


---
title: "Autoregressive models and simulation: exercise"
author: "Ben Bolker"
date:  "`r format(Sys.time(), '%H:%M %d %B %Y')`"
---

## Goals

Work through basic model-fitting and parametric bootstrapping yourself,
R coding reminders.  Similar to class notes from Thursday, but less
slick and more pedagogical.

## Topics

* semi-simple autoregressive models
* tips for statistical computing
* parametric bootstrapping


## Semi-simple AR models

Run this code to let current versions of 
R know about `sigma`, which is added in the
development version of R: from the [NEWS file](http://stat.ethz.ch/R-manual/R-devel/doc/html/NEWS.html),

> New S3 generic function sigma() with methods for extracting the estimated standard deviation aka “residual standard deviation” from a fitted model. 

```{r setup}
if (getRversion() < '3.3.0') {
    sigma <- function(object, ...) UseMethod("sigma")
    sigma.default <- function(object,...) {
        summary(object)$sigma
    }
}
```

It's good practice to load all required packages at the
head of your code; it's sometimes frustrating to get
most of the way into an analysis and then have to 
stop to install packages.  Remember that you only
have to install packages once per R installation
(re-installation is only needed after "major" releases,
e.g. 3.1 $to$ 3.2).

Load data from Dennis and Taper on Yellowstone grizzlies.
To load data you need to set your directory 
structure and *working directory* properly (this is often
the hardest part of an introductory R session!).  

You can either: 

* set up your directories (folders) for class materials to mirror
the Github repository, with a `data` directory parallel to your
working directory (which on the repository is `notes`), *or*
* keep the data and R scripts in the same file and modify the
file below to `"DTgrizzly.txt"`.
* The simplest way to set the working directory is with the `Session >
Set Working Directory > To Source File Location` menu item;
you can also use `setwd()` from the console.

```{r getdata}
grizzly <- read.table("../data/DTgrizzly.txt",header=TRUE)
```

* **Question**: what happens if you forget `header=TRUE`?
Unless you know the answer already, try it, and use `str(badgrizzly)`
to check out the result.
(One good reason to keep your data in CSV rather than space-separated
files, besides the fact that they can be exported from Excel and
can handle strings containing spaces, is that `read.csv()`
uses `header=TRUE` by default.)

Lag the variables by dropping the first and
last elements respectively. 
```{r lags}
nlag1 <- grizzly$n[-1]
nlag0 <- grizzly$n[-length(grizzly$n)]
r <- log(nlag1/nlag0)
```

* **Q**: what happens if you forget to make `nlag0` and
just compute `nlag1/grizzly$n`?
(If all this is new to you, *stop and print the values of
`nlag0` and `nlag1`. It may be helpful to align
them using `data.frame(nlag0,nlag1)`.)

Look at the data:
```{r plot1,fig.keep="none"}
plot(n~time,data=grizzly,type="b",xlab="time",ylab="pop size")
plot(nlag0,r,xlab="pop size",ylab="logarithmic growth rate")
```

Put the data together and run the linear regression:
```{r transform}
dd <- data.frame(x=log(nlag0),xlag1=log(nlag1))
m2 <- lm(xlag1~offset(x)+exp(x),data=dd)
```

Use `coef()` and `summary()` to look at the results.

Stop and convince yourself that the results are
identical if you subtract `xlag1` from both sides of 
the equation (i.e., don't use `offset`).

Define the MLE variance, which is $\mbox{SSQ}/n = \sigma^2_{\textrm{unbiased}} \cdot (n-2)/n$:
```{r var.mle}
var.mle <- function(m)
    sigma(m)^2*df.residual(m)/nobs(m)
```

Simplified model:
```{r simpmodel}
## R tip: use update() when possible to simplify code
m1 <- update(m2, . ~ offset(x))
c(coef(m1),var.mle(m1))
```

* **Question**: confirm that this is equivalent to fitting
the simple model from scratch.

## Parametric bootstrapping

## Simulation

Define simulation code. (Not too much to say here.  It might make
the function look a little bit simpler to use parameters
`{a,b,sigma,x0}` rather than `{fit,x0}`, but then we would have
to do all the extraction of coefficients outside of the
function anyway ...)


```{r simfun}
## simulate new data, add starting value
simfun <- function(fit,x0=log(grizzly$n[1])) {
    cc <- coef(fit)
    a <- cc[1]
    ## make the function work for either constant or regression model
    b <- if (length(cc)==2) coef(fit)[2] else 0
    sigma <- sqrt(var.mle(fit))
    ## as usual, half the code is setup: the next three lines are
    ##   the actual meat
    res <- c(x0,numeric(nobs(fit)+1))
    for (i in 2:length(res)) {
        res[i] <- res[i-1]+a+b*exp(res[i-1])+rnorm(1,sd=sigma)
    }
    res
}
```

Test simulation function:
```{r plotsim1,cache=TRUE}
sim2 <- replicate(200,simfun(m2))
sim1 <- replicate(200,simfun(m1))
```

`replicate()` is convenient, but may feel too magical.
(I always get confused because the syntax is `replicate(n,expression)`,
whereas `replicate(expression,n)` would seem more natural.
[What happens if you type `replicate(simfun(m2),200)` ?])

```{r plotsim2,fig.width=6,fig.height=6}
plot(log(grizzly$n),col="red",ylim=c(3,5.5))
matlines(sim2,col="black",lty=1)
matlines(sim1,col="blue",lty=1)
```

If you feel like it, add a legend ...

Here is a less-magical way to run the parametric bootstrap:
```{r pb1,cache=TRUE}
nboot <- 2000
res <- matrix(NA,nrow=nboot,ncol=3)
set.seed(101)
for (i in 1:nboot) {
    x <- simfun(m2)
    dd <- data.frame(xlag1=x[-1],x=x[-length(x)])
    bootfit <- update(m2,data=dd)
    res[i,] <- c(coef(bootfit),var.mle(bootfit))
}
```    

```{r plotpb1}
par(mfrow=c(1,3))
hist(res[,1],col="gray")
hist(res[,2],col="gray")
hist(res[,3],col="gray")
```

```{r confint}
t(apply(res,2,quantile,c(0.025,0.975)))
```

Compare this with the `confint()` values for the original model.

Now suppose we want to do a hypothesis test.

We can retrieve the $F$ statistic for the difference
between models this way:

```{r anova}
(a1 <- anova(m1,m2))
F_obs <- a1[2,"F"]
```

```{r pboot2,cache=TRUE}
nboot <- 2000
res2 <- numeric(nboot)
set.seed(101)
for (i in 1:nboot) {
    x <- simfun(m1)
    dd <- data.frame(xlag1=x[-1],x=x[-length(x)])
    bootm1 <- update(m1,data=dd)
    bootm2 <- update(m2,data=dd)
    res2[i] <- anova(bootm1,bootm2)[2,"F"]
}
```


```{r plot_pboot2}
hist(res2,col="gray",freq=FALSE,breaks=50)
curve(df(x,1,14),add=TRUE,col="red")
abline(v=F_obs,col="blue")
sum(res2>=F_obs)/length(res2)
## DT get 0.72 instead ...
```
We can also use `mean(res2>=F_obs)` as a shortcut
for `sum(res2>=F_obs)/length(res2)`.

## If you're finished/bored ...

* re-do the analysis for the elk data set in `notes/DTelk.txt`.
Dennis and Taper give the results as
   * model 1 (density-independent): $a=0.0145$, $\sigma^2=0.0685$
   * model 2 (density-dependent): $a=0.731$, $b=-4.93 \times 10^{-4}$, $\sigma^2=0.0465$
   * $\hat P = 0.044$
* write a general function to do these PB tests   
* Try the parametric bootstrap using the likelihood ratio test (e.g., see `anova(m1,m2,test="Chisq")`) instead.  Why is the answer different?
* Explore alternate models for the data.  The *Gompertz state-space model*
uses $X_t=X_{t-1} + a+ b X_{t-1} + \sigma Z_t$ (i.e., substituting $bX_{t-1}$ for $b \exp(X_{t-1}$). Another alternative would be a *generalized Ricker*, using $X_t = (c+1) X_{t-1} + a + b \exp{X_t-1} + \sigma Z_t$ ...


