---
title: "Inference geometry"
author: "Ben Bolker"
date:  "`r format(Sys.time(), '%H:%M %d %B %Y')`"
output: rmarkdown::tufte_handout
bibliography: nonlin.bib
header-includes:
   - \\usepackage[utf8]{inputenc}
   - \\hypersetup{colorlinks}
---

This document is supposed to illustrate the variety of approaches
to computing interval estimates (i.e., confidence intervals) in
frequentist and Bayesian frameworks, and especially their geometric
interpretations.

```{r pkgs,message=FALSE}
library("bbmle")
library("emdbook")
library("ellipse")
library("viridis")
library("MCMCpack")
```

```{r knitopts,echo=FALSE}
library("knitr")
opts_chunk$set(echo=FALSE)
```

## Normal data

```{r ex1}
n <- 5
set.seed(101)
z <- rnorm(n)
m0 <- mle2(z~dnorm(mu,sd=sigma),
           start=list(mu=0,sigma=1),
           data=data.frame(z),
           method="L-BFGS-B",
           lower=c(mu=-5,sigma=0.002))
cc <- curve3d(m0@minuslogl(x,y),
              xlim=c(-0.7,0.7),ylim=c(0.1,1.5),
              n=c(61,61),sys3d="none")
with(cc,image(x,y,z))
pp <- profile(m0)
p1 <- pp@profile[[1]]$par.vals
p2 <- pp@profile[[2]]$par.vals
with(cc,contour(x,y,z,add=TRUE,levels=-logLik(m0)+
                    qchisq(levels,df=2)/2,
                labels=levels))

```
```{r setup}
n <- 20  ## originally 20
set.seed(101)
x <- rnorm(n)
y <- rnbinom(n,mu=1,size=exp(1+0.5*x))
nll <- function(a,b) {
    -sum(dnbinom(y,mu=1,size=exp(a+b*x),log=TRUE))
}
m1 <- mle2(y~dnbinom(mu=1,size=exp(logk)),
      parameters=list(logk~x),
      start=c(logk=0),
      data=data.frame(x,y),
     method="BFGS")
levels <- c(0.5,0.95,0.99)
cc <- curve3d(nll,xlim=c(-3,5),ylim=c(-4,4),
              n=c(61,61),sys3d="none")
image(cc$x,cc$y,cc$z,
      col=viridis(20),useRaster=TRUE)

```
```{r prof,cache=TRUE,warning=FALSE}
pp <- profile(m1,std.err=0.1,alpha=0.01,maxsteps=1000)
p1 <- pp@profile[[1]]$par.vals
p2 <- pp@profile[[2]]$par.vals
```

```{r}
dx <- diff(cc$x)[1]
dy <- diff(cc$y)[1]
## not sure I have this computation right!
## where do dx, dy go??
scvals <- exp(-cc$z)/sum(exp(-cc$z)) ## probabilities
scvals2 <- cumsum(sort(scvals))
w <- sapply(levels,function(x) { which(scvals2>1-x)[1] })
critval <- sort(scvals)[w]  ## val
## sum(scvals[scvals>critval[3]])

## various attempts at MCMC
if (FALSE) {
    mm <- MCMCmetrop1R(ll2,coef(m1),
                       V=vcov(m1),mcmc=5e6,thin=1000,
                       seed=1001,tune=0.01)
    mm2 <- as.matrix(mm)
}
```

```{r getstan,fig.keep="none"}
stanout <- read.csv("stan_out.csv")
h <- lapply(levels,
            function(x) HPDregionplot(stanout,prob=x))
```

```{r output,fig.width=6,fig.height=6}
image(cc$x,cc$y,cc$z,
      col=viridis(20),useRaster=TRUE)
with(cc,contour(x,y,z,add=TRUE,levels=-logLik(m1)+
                    qchisq(levels,df=2)/2,
                labels=levels))
with(cc,contour(x,y,scvals,add=TRUE,level=critval,
                labels=levels,col=2))
## 95% level == 99% level? coincide exactly??
lapply(h,function(x) lapply(x,lines,col="pink"))
invisible(lapply(levels,function(L) lines(ellipse(vcov(m1),centre=coef(m1),level=L),lty=2)))
## profiles
lines(p1[,1],p1[,2],col="blue")
lines(p2[,1],p2[,2],col="purple")
```


## TO DO

* clean up!
* justify 50% and 99% CIs
* add Stan results (better/redo)
* other MCMC results?
* univariate/marginal views
* brute-force integration
* cf. NBinom example (explain motivation) and a t-distribution example
